{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"##### from torch.utils.data import Dataset\nimport random\nimport torch\nimport glob as gl\nimport os\nimport random\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import InterpolationMode\nimport cv2\n#定义数据集\nclass ImageDataset(Dataset):\n    def __init__(self,root='',trans=None):\n        super().__init__()\n        self.transform = transforms.Compose(trans)\n        self.A_path = os.path.join(root,\"trainA/*\")\n        self.B_path = os.path.join(root,\"trainB/*\")\n        self.listA = gl.glob(self.A_path)\n        self.listB = gl.glob(self.B_path)\n    def  __getitem__ (self, index):\n        imgA_path = random.choice(self.listA)\n        imgB_path = random.choice(self.listB)\n        imgA = Image.open(imgA_path).convert('RGB')\n        imgB = Image.open(imgB_path).convert('RGB')\n        img_A = self.transform(imgA)\n        img_B = self.transform(imgB)\n        return img_A,img_B\n    def __len__(self):\n        return max(len(self.listA),len(self.listB)) \nclass ImageDataset_test(Dataset):\n    def __init__(self,root='',trans=None):\n        super().__init__()\n        self.transform = transforms.Compose(trans)\n        self.A_path = os.path.join(root,\"testA/*\")\n        self.B_path = os.path.join(root,\"testB/*\")\n        self.listA = gl.glob(self.A_path)\n        self.listB = gl.glob(self.B_path)\n    def  __getitem__ (self, index):\n        imgA_path = random.choice(self.listA)\n        imgB_path = random.choice(self.listB)\n        imgA = Image.open(imgA_path).convert('RGB')\n        imgB = Image.open(imgB_path).convert('RGB')\n        img_A = self.transform(imgA)\n        img_B = self.transform(imgB)\n        return img_A,img_B\n    def __len__(self):\n        return max(len(self.listA),len(self.listB))     ","metadata":{"execution":{"iopub.status.busy":"2022-11-30T01:28:38.172039Z","iopub.execute_input":"2022-11-30T01:28:38.172421Z","iopub.status.idle":"2022-11-30T01:28:38.188530Z","shell.execute_reply.started":"2022-11-30T01:28:38.172386Z","shell.execute_reply":"2022-11-30T01:28:38.185515Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import Parameter\n#定义残差快\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torchvision.transforms import InterpolationMode\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\nimport time as t\nclass ResnetBlock(nn.Module):\n    def __init__(self,channels,bias):\n        super(ResnetBlock, self).__init__()\n        self.Resblock=[]\n        self.Resblock+=[nn.ReflectionPad2d(1),\n                       nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=0, bias=False),\n                       nn.InstanceNorm2d(channels),\n                       nn.ReLU(True)]\n        self.Resblock+=[nn.ReflectionPad2d(1),\n                       nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=0, bias=False),\n                       nn.InstanceNorm2d(channels)]\n        self.conv_block=nn.Sequential(*self.Resblock)\n    def forward(self,x):\n        return x+self.conv_block(x)\nclass ResnetAdaILNBlock(nn.Module):\n    def __init__(self, dim, use_bias):\n        super(ResnetAdaILNBlock, self).__init__()\n        self.pad1 = nn.ReflectionPad2d(1)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias)\n        self.norm1 = adaILN(dim)\n        self.relu1 = nn.ReLU(True)\n        self.pad2 = nn.ReflectionPad2d(1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias)\n        self.norm2 = adaILN(dim)\n\n    def forward(self, x, gamma, beta):\n        out = self.pad1(x)\n        out = self.conv1(out)\n        out = self.norm1(out, gamma, beta)\n        out = self.relu1(out)\n        out = self.pad2(out)\n        out = self.conv2(out)\n        out = self.norm2(out, gamma, beta)\n        return out + x\nclass adaILN(nn.Module):\n    def __init__(self, num_features, eps=1e-5):\n        super(adaILN, self).__init__()\n        self.eps = eps\n        self.rho = Parameter(torch.Tensor(1, num_features, 1, 1))\n        self.rho.data.fill_(0.9)\n\n    def forward(self, input, gamma, beta):\n        in_mean, in_var = torch.mean(input, dim=[2, 3], keepdim=True), torch.var(input, dim=[2, 3], keepdim=True)\n        out_in = (input - in_mean) / torch.sqrt(in_var + self.eps)\n        ln_mean, ln_var = torch.mean(input, dim=[1, 2, 3], keepdim=True), torch.var(input, dim=[1, 2, 3], keepdim=True)\n        out_ln = (input - ln_mean) / torch.sqrt(ln_var + self.eps)\n        out = self.rho.expand(input.shape[0], -1, -1, -1) * out_in + (1-self.rho.expand(input.shape[0], -1, -1, -1)) * out_ln\n        out = out * gamma.unsqueeze(2).unsqueeze(3) + beta.unsqueeze(2).unsqueeze(3)\n        return out\nclass ILN(nn.Module):\n    def __init__(self, num_features, eps=1e-5):\n        super(ILN, self).__init__()\n        self.eps = eps\n        self.rho = Parameter(torch.Tensor(1, num_features, 1, 1))\n        self.gamma = Parameter(torch.Tensor(1, num_features, 1, 1))\n        self.beta = Parameter(torch.Tensor(1, num_features, 1, 1))\n        self.rho.data.fill_(0.0)\n        self.gamma.data.fill_(1.0)\n        self.beta.data.fill_(0.0)\n\n    def forward(self, input):\n        in_mean, in_var = torch.mean(input, dim=[2, 3], keepdim=True), torch.var(input, dim=[2, 3], keepdim=True)\n        out_in = (input - in_mean) / torch.sqrt(in_var + self.eps)\n        ln_mean, ln_var = torch.mean(input, dim=[1, 2, 3], keepdim=True), torch.var(input, dim=[1, 2, 3], keepdim=True)\n        out_ln = (input - ln_mean) / torch.sqrt(ln_var + self.eps)\n        out = self.rho.expand(input.shape[0], -1, -1, -1) * out_in + (1-self.rho.expand(input.shape[0], -1, -1, -1)) * out_ln\n        out = out * self.gamma.expand(input.shape[0], -1, -1, -1) + self.beta.expand(input.shape[0], -1, -1, -1)\n        return out\n\n#定义生成器\nclass Generator(nn.Module):\n    def __init__(self,light=False,num_block=4):\n        super(Generator,self).__init__()\n        self.light=light\n        self.num_block=num_block\n        channels=[3,64,128,256,512]\n        #编码器部分\n        self.DownBlock=[]\n        self.DownBlock+=[nn.ReflectionPad2d(3),nn.Conv2d(channels[0],channels[1],7,1,0, bias=False),\n                  nn.InstanceNorm2d(channels[1]),nn.ReLU(True)]\n        #down 下采样 2次\n        for i in range(1,len(channels)-2):\n            self.DownBlock+=[nn.ReflectionPad2d(1),nn.Conv2d(channels[i],channels[i+1],3,2,0, bias=False),\n                      nn.InstanceNorm2d(channels[i+1]),nn.ReLU(True)]\n        #定义残差块\n        for j in range(self.num_block):\n            self.DownBlock+=[ResnetBlock(256,bias=False)]\n        #CAM\n        self.gap_fc = nn.Linear(channels[3], 1, bias=False)\n        self.gmp_fc = nn.Linear(channels[3], 1, bias=False)\n        self.conv1x1=nn.Conv2d(channels[4],channels[3],1,1,bias=True)#512--256\n        self.relu=nn.ReLU(True)\n        #gamma,beta block\n        self.FC = []\n        if self.light:\n            self.FC=[nn.Linear(channels[3],channels[3],bias=False),\n                nn.ReLU(True),nn.Linear(channels[3],channels[3],bias=False),\n                nn.ReLU(True)]\n        else:\n            self.FC=[nn.Linear(channels[1]**2*channels[3],channels[3],bias=False),\n                nn.ReLU(True),\n                nn.Linear(channels[3], channels[3], bias=False),\n                nn.ReLU(True)]\n        self.gamma = nn.Linear(channels[3],channels[3], bias=False)\n        self.beta = nn.Linear(channels[3], channels[3], bias=False)\n        #解码器\n        # ADALIN\n        self.UpBlock2 =[]\n        for i in range(num_block):\n            setattr(self, 'UpBlock1_' + str(i+1), ResnetAdaILNBlock(channels[3],use_bias=False))\n        # up samlping\n        channels.reverse()\n        n=1\n        for m in range(1,len(channels)-2):\n            self.UpBlock2+=[nn.Upsample(scale_factor=2,mode='nearest'),#上采样的算法\n                     nn.ReflectionPad2d(1),\n                     nn.Conv2d(channels[m],channels[m+1],3,1,0,bias=False),\n                     ILN(channels[m+1]),nn.ReLU(True)]\n            n+=1\n        self.UpBlock2+=[nn.ReflectionPad2d(3),nn.Conv2d(channels[n],channels[n+1], kernel_size=7, stride=1, padding=0, bias=False),\n                     nn.Tanh()]\n        self.DownBlock=nn.Sequential(*self.DownBlock)\n        self.FC=nn.Sequential(*self.FC)\n        self.UpBlock2=nn.Sequential(*self.UpBlock2)\n    def forward(self,x):\n        x=self.DownBlock(x)\n        #输入全连接层\n        gap=nn.functional.adaptive_avg_pool2d(x,1)\n        gap_out=self.gap_fc(gap.view(x.shape[0],-1))\n        gap_weight=list(self.gap_fc.parameters())[0]\n        \n        gap=x*gap_weight.unsqueeze(2).unsqueeze(3)\n\n        gmp = nn.functional.adaptive_max_pool2d(x, 1)\n        gmp_out = self.gmp_fc(gmp.view(x.shape[0], -1))\n        gmp_weight = list(self.gmp_fc.parameters())[0]\n        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)\n        #CAM\n        cam_logit=torch.cat([gap_out,gmp_out],1)\n        x=torch.cat([gap,gmp],1)\n        x=self.relu(self.conv1x1(x))\n        heatmap = torch.sum(x, dim=1, keepdim=True)\n        if self.light:\n            x_ = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n            x_ = self.FC(x_.view(x_.shape[0], -1))\n        else:\n            x_ = self.FC(x.view(x.shape[0], -1))\n        gamma,beta = self.gamma(x_), self.beta(x_)\n        for i in range(self.num_block):\n            x = getattr(self, 'UpBlock1_' + str(i+1))(x,gamma,beta)\n        out=self.UpBlock2(x)\n        return out,cam_logit,heatmap\n#判别器\nclass Discriminator(nn.Module):\n    def __init__(self,n_layers=5):\n        super(Discriminator,self).__init__()\n        if n_layers==5:\n            channels=[3,64,128,256,512,1024]\n        else:\n            channels=[3,64,128,256,512,1024,2048,4096]\n        #编码器\n        self.model=[]\n        n=0\n        for i in range(len(channels)-3):\n            self.model+=[nn.ReflectionPad2d(1),nn.utils.spectral_norm(\n            nn.Conv2d(channels[i],channels[i+1],4,2,0,bias=True)),\n                     nn.LeakyReLU(0.2,True)]\n            n+=1\n        self.model+=[nn.ReflectionPad2d(1),nn.utils.spectral_norm(\n            nn.Conv2d(channels[n],channels[n+1],4,1,0,bias=True)),\n                     nn.LeakyReLU(0.2,True)]\n          # Class Activation Map\n        self.gap_fc=nn.utils.spectral_norm(nn.Linear(channels[n+1],1,bias=False))\n        self.gmp_fc=nn.utils.spectral_norm(nn.Linear(channels[n+1],1,bias=False))\n        self.conv1x1=nn.Conv2d(channels[n+2],channels[n+1],1,1,bias=True)\n        self.leaky_relu=nn.LeakyReLU(0.2,True)\n        #classifcation\n        self.pad = nn.ReflectionPad2d(1)\n        self.conv = nn.utils.spectral_norm(\n            nn.Conv2d(channels[n+1], 1, kernel_size=4, stride=1, padding=0, bias=False))\n\n        self.model = nn.Sequential(*self.model)\n\n        # Class Activation Map\n\n    def forward(self,x):\n        x=self.model(x)\n        gap = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n        gap_logit = self.gap_fc(gap.view(x.shape[0], -1))\n        gap_weight = list(self.gap_fc.parameters())[0]\n        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)\n\n        gmp = torch.nn.functional.adaptive_max_pool2d(x, 1)\n        gmp_logit = self.gmp_fc(gmp.view(x.shape[0], -1))\n        gmp_weight = list(self.gmp_fc.parameters())[0]\n        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)\n\n        cam_logit = torch.cat([gap_logit, gmp_logit],1)\n        x = torch.cat([gap, gmp],1)\n        x = self.leaky_relu(self.conv1x1(x))\n        heatmap = torch.sum(x, dim=1, keepdim=True)\n        x = self.pad(x)\n        x=self.conv(x)\n        return x, cam_logit, heatmap\n\nclass RhoCliper(object):\n        def __init__(self, min, max):\n            self.clip_min = min\n            self.clip_max = max\n            assert min < max\n\n        def __call__(self, module):\n            if hasattr(module, 'rho'):\n                w = module.rho.data\n                w = w.clamp(self.clip_min, self.clip_max)\n                module.rho.data = w","metadata":{"execution":{"iopub.status.busy":"2022-11-30T01:28:38.190910Z","iopub.execute_input":"2022-11-30T01:28:38.191469Z","iopub.status.idle":"2022-11-30T01:28:38.365547Z","shell.execute_reply.started":"2022-11-30T01:28:38.191434Z","shell.execute_reply":"2022-11-30T01:28:38.364561Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def denorm(x):\n    return x * 0.5 + 0.5\n\ndef tensor2numpy(x):\n    return x.detach().cpu().numpy().transpose(1,2,0)\n\ndef RGB2BGR(x):\n    return cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n\ndef change_time(time):\n    new_time=t.localtime(time)\n    new_time=t.strftime(\"%Hh%Mm%Ss\",new_time)\n    return new_time\ndef check_folder(log_dir):\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    return log_dir\n\ndef str2bool(x):\n    return x.lower() in ('true')\n\ndef cam(x, size = 256):\n    x = x - np.min(x)\n    cam_img = x / np.max(x)\n    cam_img = np.uint8(255 * cam_img)\n    cam_img = cv2.resize(cam_img, (size, size))\n    cam_img = cv2.applyColorMap(cam_img, cv2.COLORMAP_JET)\n    return cam_img / 255.0\ndef weights_init(m):                                    \n    classname = m.__class__.__name__                        \n    if classname.find(\"Conv\") != -1:                        ## find():实现查找classname中是否含有Conv字符，没有返回-1；有返回0.\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)     ## m.weight.data表示需要初始化的权重。nn.init.normal_():表示随机初始化采用正态分布，均值为0，标准差为0.02.\n        if hasattr(m, \"bias\") and m.bias is not None:       ## hasattr():用于判断m是否包含对应的属性bias, 以及bias属性是否不为空.\n            torch.nn.init.constant_(m.bias.data, 0.0)       ## nn.init.constant_():表示将偏差定义为常量0.\n    elif classname.find(\"BatchNorm2d\") != -1:               ## find():实现查找classname中是否含有BatchNorm2d字符，没有返回-1；有返回0.\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)     ## m.weight.data表示需要初始化的权重. nn.init.normal_():表示随机初始化采用正态分布，均值为0，标准差为0.02.\n        torch.nn.init.constant_(m.bias.data, 0.0) \n#自定义学习率衰退\ndef MyLambdaLR(lr,iter):\n    i=0\n    lr-=(lr/(iter//2))\n    return lr\n        \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-30T01:28:38.366987Z","iopub.execute_input":"2022-11-30T01:28:38.367435Z","iopub.status.idle":"2022-11-30T01:28:38.381325Z","shell.execute_reply.started":"2022-11-30T01:28:38.367398Z","shell.execute_reply":"2022-11-30T01:28:38.380359Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import time, itertools\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom glob import glob\nfrom PIL import Image\nfrom torch import  nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imsave\nimport  numpy as np\nfrom torchvision import utils\nfrom tqdm import tqdm\n#定义UGATIT\nclass UGATIT(object):\n    def __init__(self,args):\n        self.light=args.light\n        self.dataset=args.dataset\n        self.mode=args.mode\n         #config\n        self.batch_size=args.batch_size\n        self.iter = args.iter\n        self.lr=args.lr\n        self.decay_lr=args.decay_lr\n        self.device=args.device\n        self.b1=args.b1\n        self.b2=args.b2\n        self.path=args.path\n        self.data_path=args.data_path\n        self.test_path=args.test_path\n\n         # Weight \"\"\"\n        self.adv_weight = args.adv_weight\n        self.cycle_weight = args.cycle_weight\n        self.identity_weight = args.identity_weight\n        self.cam_weight = args.cam_weight\n        self.weight_decay=args.weight_decay\n        #img\n        self.img_size=args.img_size\n        self.channels=args.channels\n        self.dir=args.dir\n        self.print_freq=args.print_freq\n        #模型\n        self.G_A2B=Generator(light=self.light).to(self.device)\n        self.G_B2A=Generator(light=self.light).to(self.device)\n        self.D_GA=Discriminator(n_layers=7).to(self.device)\n        self.D_GB=Discriminator(n_layers=7).to(self.device)\n        self.D_LA=Discriminator().to(self.device)\n        self.D_LB=Discriminator().to(self.device)\n        #定义损失函数\n        self.L1_loss = nn.L1Loss().to(self.device)\n        self.MSE_loss = nn.MSELoss().to(self.device)\n        self.BCE_loss = nn.BCEWithLogitsLoss().to(self.device)\n        #定义优化器\n        self.G_optim=optim.Adam(itertools.chain(self.G_A2B.parameters(),self.G_B2A.parameters()),lr=self.decay_lr,\n                                                betas=(self.b1,self.b2),weight_decay=self.weight_decay)\n        self.D_optim=optim.Adam(itertools.chain(self.D_GA.parameters(),self.D_GB.parameters(),self.D_LA.parameters(),\n                                                self.D_LB.parameters()),lr=self.decay_lr,\n                                        betas=(self.b1,self.b2),weight_decay=self.weight_decay)\n        #定义学习率策略\n#         self.G_lr_scheduler=lr_scheduler.LambdaLR(self.G_optim,lr_lambda=MyLambdaLR(self.lr,self.iter))\n#         self.D_lr_scheduler=lr_scheduler.LambdaLR(self.D_optim,lr_lambda=MyLambdaLR(self.lr,self.iter))\n        #rho\n        self.rho=RhoCliper(0, 1)\n    #读取数据集\n    def load_data(self):\n        train_transform =[\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.Resize((self.img_size + 30, self.img_size + 30)),\n            transforms.RandomCrop(self.img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n        ]\n        test_transform =[\n            transforms.Resize((self.img_size, self.img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n        ]\n\n        self.train_iter=DataLoader(ImageDataset(os.path.join(self.data_path,self.dataset),trans=train_transform),batch_size=self.batch_size,\n                              shuffle=False)\n        self.test_iter=DataLoader(ImageDataset_test(os.path.join(self.data_path,self.dataset),trans=test_transform),batch_size=self.batch_size,\n                              shuffle=False)\n        # 保存模型\n\n    def save_model(self):\n        params = {}\n        params[\"genA2B\"] = self.G_A2B.state_dict()\n        params[\"genB2A\"] = self.G_B2A.state_dict()\n        params[\"disGA\"] = self.D_GA.state_dict()\n        params[\"disGB\"] = self.D_GB.state_dict()\n        params[\"disLA\"] = self.D_LA.state_dict()\n        params[\"disLB\"] = self.D_LB.state_dict()\n        torch.save(params, os.path.join(self.dir, self.dataset + \"model.pt\"))\n#     def save_lr(self):\n#         lr_pamars={}\n#         params['G_lr']=self.G_optim.state_dict()\n#         params['D_lr']=self.D_optim.state_dict()\n#         torch.save(lr_params,os.path.join(self.dir,self.dataset+'lr.pt'))\n        \n        #加载模型\n    def load_model(self):\n        params=torch.load(os.path.join(self.test_path,'selfie2animemodel.pt'))\n        \n        self.G_A2B.load_state_dict(params['genA2B'])\n        self.G_B2A.load_state_dict(params['genB2A'])\n        self.D_GA.load_state_dict(params['disGA'])\n        self.D_GB.load_state_dict(params['disGB'])\n        self.D_LA.load_state_dict(params['disLA'])\n        self.D_LB.load_state_dict(params['disLB'])\n        print(\"加载模型成功！\")     \n#         self.G_optim.load_state_dict(lr_params['G_lr'])\n#         self.D_optim.load_state_dict(lr_params['D_lr'])\n        print(\"加载学习率成功！\")   \n    #训练\n    def train(self):\n        #训练模式\n        self.G_A2B.train(),self.G_B2A.train(),self.D_GA.train(),self.D_GB.train(),\n        self.D_LA.train(),self.D_LB.train()\n        start_time=time.time()\n        for step in tqdm(range(1,10001)):\n            real_A,_=next(iter(self.train_iter))\n            _,real_B=next(iter(self.train_iter))\n            real_A,real_B=real_A.to(self.device),real_B.to(self.device)\n            #updater D\n            self.D_optim.zero_grad()\n            fake_B,_,_=self.G_A2B(real_A)\n            fake_A,_,_=self.G_B2A(real_B)\n            #forward\n            real_logit_ga,real_cam_logit_ga,_=self.D_GA(real_A)\n            real_logit_la,real_cam_logit_la,_=self.D_LA(real_A)\n            real_logit_gb,real_cam_logit_gb,_=self.D_GB(real_B)\n            real_logit_lb,real_cam_logit_lb,_=self.D_LB(real_B)\n\n\n            fake_logit_ga,fake_cam_logit_ga,_=self.D_GA(fake_A)\n            fake_logit_la,fake_cam_logit_la,_=self.D_LA(fake_A)\n            fake_logit_gb,fake_cam_logit_gb,_=self.D_GB(fake_B)\n            fake_logit_lb,fake_cam_logit_lb,_=self.D_LB(fake_B)\n\n            #loss A\n            D_loss_GA=self.MSE_loss(real_logit_ga,torch.ones_like(real_logit_ga).to(self.device))+self.MSE_loss(fake_logit_ga,torch.zeros_like(fake_logit_ga).to(self.device))\n            D_loss_cam_GA=self.MSE_loss(real_cam_logit_ga,torch.ones_like(real_cam_logit_ga).to(self.device))+self.MSE_loss(fake_cam_logit_ga,torch.zeros_like(fake_cam_logit_ga).to(self.device))\n            D_loss_LA=self.MSE_loss(real_logit_la,torch.ones_like(real_logit_la).to(self.device))+self.MSE_loss(fake_logit_la,torch.zeros_like(fake_logit_la)).to(self.device)\n            D_loss_cam_LA=self.MSE_loss(real_cam_logit_la,torch.ones_like(real_cam_logit_la).to(self.device))+self.MSE_loss(fake_cam_logit_la,torch.zeros_like(fake_cam_logit_la).to(self.device))\n            D_loss_GB=self.MSE_loss(real_logit_gb,torch.ones_like(real_logit_gb).to(self.device))+self.MSE_loss(fake_logit_gb,torch.zeros_like(fake_logit_gb).to(self.device))\n            D_loss_cam_GB=self.MSE_loss(real_cam_logit_gb,torch.ones_like(real_cam_logit_gb).to(self.device))+self.MSE_loss(fake_cam_logit_gb,torch.zeros_like(fake_cam_logit_gb).to(self.device))\n            D_loss_LB=self.MSE_loss(real_logit_lb,torch.ones_like(real_logit_lb).to(self.device))+self.MSE_loss(fake_logit_lb,torch.zeros_like(fake_logit_lb).to(self.device))\n            D_loss_cam_LB=self.MSE_loss(real_cam_logit_lb,torch.ones_like(real_cam_logit_lb).to(self.device))+self.MSE_loss(fake_cam_logit_lb,torch.zeros_like(fake_cam_logit_lb).to(self.device))\n            #D_loss\n            D_loss_A=self.adv_weight*(D_loss_GA+D_loss_cam_GA+D_loss_LA+D_loss_cam_LA)\n            D_loss_B=self.adv_weight*(D_loss_GB+D_loss_cam_GB+D_loss_LB+D_loss_cam_GB)\n            D_loss=(D_loss_A+D_loss_B)\n            D_loss.backward()\n            self.D_optim.step()\n            #Update G\n            self.G_optim.zero_grad()\n\n            fake_a2b,fake_cam_logit_a2b,_=self.G_A2B(real_A)\n            fake_b2a,fake_cam_logit_b2a,_=self.G_B2A(real_B)\n            \n            fake_a2b2a,_,_=self.G_B2A(fake_a2b)\n            fake_b2a2b,_,_=self.G_A2B(fake_b2a)\n            \n            fake_a2a, fake_cam_logit_a2a, _ = self.G_B2A(real_A)\n            fake_b2b, fake_cam_logit_b2b, _ = self.G_A2B(real_B)\n\n            fake_logit_ga,fake_cam_logit_ga,_=self.D_GA(fake_b2a)\n            fake_logit_la,fake_cam_logit_la,_=self.D_LA(fake_b2a)\n            fake_logit_gb,fake_cam_logit_gb,_=self.D_GB(fake_a2b)\n            fake_logit_lb,fake_cam_logit_lb,_=self.D_LB(fake_a2b)\n            #adv loss\n            loss_adv_Ga2b=self.MSE_loss(fake_logit_gb,torch.ones_like(fake_logit_gb).to(self.device))\n            loss_adv_cam_Ga2b=self.MSE_loss(fake_cam_logit_gb,torch.ones_like(fake_cam_logit_gb).to(self.device))\n            loss_adv_La2b=self.MSE_loss(fake_logit_lb,torch.ones_like(fake_logit_lb).to(self.device))\n            loss_adv_cam_La2b=self.MSE_loss(fake_cam_logit_lb,torch.ones_like(fake_cam_logit_lb).to(self.device))\n            loss_adv_Gb2a = self.MSE_loss(fake_logit_ga, torch.ones_like(fake_logit_ga).to(self.device))\n            loss_adv_cam_Gb2a = self.MSE_loss(fake_cam_logit_ga, torch.ones_like(fake_cam_logit_ga).to(self.device))\n            loss_adv_Lb2a = self.MSE_loss(fake_logit_la, torch.ones_like(fake_logit_la).to(self.device))\n            loss_adv_cam_Lb2a = self.MSE_loss(fake_cam_logit_la, torch.ones_like(fake_cam_logit_la).to(self.device))\n            #cycle loss\n            loss_cycle_b2a=self.L1_loss(fake_a2b2a,real_A).to(self.device)\n            loss_cycle_a2b=self.L1_loss(fake_b2a2b,real_B).to(self.device)\n            #idetity loss\n            loss_identity_a2a=self.L1_loss(fake_a2a,real_A).to(self.device)\n            loss_identity_b2b=self.L1_loss(fake_b2b,real_B).to(self.device)\n            #CAM loss\n            loss_cam_A=self.BCE_loss(fake_cam_logit_b2a,torch.ones_like(fake_cam_logit_b2a).to(self.device))+self.BCE_loss(fake_cam_logit_a2a,torch.zeros_like(fake_cam_logit_a2a).to(self.device))\n            loss_cam_B=self.BCE_loss(fake_cam_logit_a2b,torch.ones_like(fake_cam_logit_a2b).to(self.device))+self.BCE_loss(fake_cam_logit_b2b,torch.zeros_like(fake_cam_logit_b2b).to(self.device))\n            #total G loss\n            loss_G_A=self.adv_weight*(loss_adv_Gb2a+loss_adv_cam_Gb2a+loss_adv_Lb2a+loss_adv_cam_Lb2a)+self.cycle_weight*loss_cycle_b2a+self.identity_weight*loss_identity_a2a+self.cam_weight*loss_cam_A\n            loss_G_B=self.adv_weight*(loss_adv_Ga2b + loss_adv_cam_Ga2b + loss_adv_La2b + loss_adv_cam_La2b) + self.cycle_weight * loss_cycle_a2b + self.identity_weight * loss_identity_b2b + self.cam_weight * loss_cam_B\n            G_loss=loss_G_A+loss_G_B\n            G_loss.backward()\n            self.G_optim.step()\n#             if step>500000:\n            self.G_optim.param_groups[0]['lr'] -= self.lr / (self.iter // 2)\n            self.D_optim.param_groups[0]['lr'] -= self.lr / (self.iter // 2)\n            \n            #apple rho\n            self.G_A2B.apply(self.rho)\n            self.G_B2A.apply(self.rho)\n            end_step=time.time()\n            print(f\"step:[{step}/{self.iter}],G_loss:{G_loss},D_loss:{D_loss},G_lr:{self.G_optim.param_groups[0]['lr']},\"\n                  f\"D_lr:{self.D_optim.param_groups[0]['lr']},time:{change_time(end_step-start_time)}\")\n            if step % self.print_freq == 0:\n                test_sample_num =5\n                train_sample_num=5\n                A2B = np.zeros((self.img_size * 7, 0, 3))\n                B2A = np.zeros((self.img_size * 7, 0, 3))\n                self.G_A2B.eval(), self.G_B2A.eval(), self.D_GA.eval(), self.D_GB.eval(), self.D_LA.eval(), self.D_LB.eval()\n                for _ in range(train_sample_num):\n                    real_A,_= next(iter(self.train_iter))\n                    _,real_B= next(iter(self.train_iter))\n                    real_A, real_B = real_A.to(self.device), real_B.to(self.device)\n                    fake_a2b,_,fake_heatmap_a2b=self.G_A2B(real_A)\n                    fake_b2a,_,fake_heatmap_b2a=self.G_B2A(real_B)\n\n                    fake_a2b2a,_,fake_heatmap_a2b2a=self.G_B2A(fake_a2b)\n                    fake_b2a2b,_,fake_heatmap_b2a2b=self.G_A2B(fake_b2a)\n\n                    fake_a2a,_,fake_heatmap_a2a=self.G_B2A(real_A)\n                    fake_b2b,_,fake_heatmap_b2b=self.G_A2B(real_B)\n#                          \n                    A2B=np.concatenate((A2B,np.concatenate((RGB2BGR(tensor2numpy(denorm(real_A[0]))),\n                                                          cam(tensor2numpy(fake_heatmap_a2a[0]),self.img_size),\n                                                          RGB2BGR(tensor2numpy(denorm(fake_a2a[0]))),\n                                                          cam(tensor2numpy(fake_heatmap_a2b[0]),self.img_size),\n                                                          RGB2BGR(tensor2numpy(denorm(fake_a2b[0]))),\n                                                          cam(tensor2numpy(fake_heatmap_a2b2a[0]),self.img_size),\n                                                          RGB2BGR(tensor2numpy(denorm(fake_a2b2a[0])))), 0)),1)\n                    B2A= np.concatenate((B2A,np.concatenate((RGB2BGR(tensor2numpy(denorm(real_B[0]))),\n                                                             cam(tensor2numpy(fake_heatmap_b2b[0]),\n                                                                 self.img_size),\n                                                             RGB2BGR(tensor2numpy(denorm(fake_b2b[0]))),\n                                                             cam(tensor2numpy(fake_heatmap_b2a[0]),\n                                                                 self.img_size),\n                                                             RGB2BGR(tensor2numpy(denorm(fake_b2a[0]))),\n                                                             cam(tensor2numpy(fake_heatmap_b2a2b[0]),\n                                                                 self.img_size),RGB2BGR(tensor2numpy(denorm(fake_b2a2b[0])))), 0)), 1)\n                for _ in range(test_sample_num):\n                    real_A,_= next(iter(self.test_iter))\n                    _,real_B= next(iter(self.test_iter))\n                    real_A, real_B = real_A.to(self.device), real_B.to(self.device)\n                    fake_a2b,_,fake_heatmap_a2b=self.G_A2B(real_A)\n                    fake_b2a,_,fake_heatmap_b2a=self.G_B2A(real_B)\n\n                    fake_a2b2a,_,fake_heatmap_a2b2a=self.G_B2A(fake_a2b)\n                    fake_b2a2b,_,fake_heatmap_b2a2b=self.G_A2B(fake_b2a)\n\n                    fake_a2a,_,fake_heatmap_a2a=self.G_B2A(real_A)\n                    fake_b2b,_,fake_heatmap_b2b=self.G_A2B(real_B)\n#                          \n                    A2B=np.concatenate((A2B,np.concatenate((RGB2BGR(tensor2numpy(denorm(real_A[0]))),\n                                                          cam(tensor2numpy(fake_heatmap_a2a[0]),self.img_size),\n                                                          RGB2BGR(tensor2numpy(denorm(fake_a2a[0]))),\n                                                          cam(tensor2numpy(fake_heatmap_a2b[0]),self.img_size),\n                                                          RGB2BGR(tensor2numpy(denorm(fake_a2b[0]))),\n                                                          cam(tensor2numpy(fake_heatmap_a2b2a[0]),self.img_size),\n                                                          RGB2BGR(tensor2numpy(denorm(fake_a2b2a[0])))), 0)),1)\n                    B2A= np.concatenate((B2A,np.concatenate((RGB2BGR(tensor2numpy(denorm(real_B[0]))),\n                                                             cam(tensor2numpy(fake_heatmap_b2b[0]),self.img_size),\n                                                             RGB2BGR(tensor2numpy(denorm(fake_b2b[0]))),\n                                                             cam(tensor2numpy(fake_heatmap_b2a[0]),self.img_size),\n                                                             RGB2BGR(tensor2numpy(denorm(fake_b2a[0]))),\n                                                             cam(tensor2numpy(fake_heatmap_b2a2b[0]),self.img_size),\n                                                             RGB2BGR(tensor2numpy(denorm(fake_b2a2b[0])))), 0)), 1)\n                    #保存训练测试图片\n                cv2.imwrite(os.path.join(self.dir, self.dataset, 'img', 'A2B_%07d.png' % step), A2B * 255.0)\n                cv2.imwrite(os.path.join(self.dir, self.dataset, 'img', 'B2A_%07d.png' % step), B2A * 255.0)\n                self.G_A2B.train(), self.G_B2A.train(), self.D_GA.train(), self.D_GB.train(), self.D_LA.train(), self.D_LB.train()\n            if step%self.print_freq==0:\n                self.save_model()\n    \n                print(\"保存模型和学习率成功！\")\n    #测试\n    def test(self):\n        print(\"读取模型！\")\n        self.load_model()\n        self.G_A2B.eval(),self.G_B2A.eval()\n        for j in range(30):\n            for i,(x,y) in tqdm(enumerate(self.test_iter)):\n                x,y=x.to(self.device),y.to(self.device)\n                fake_A2B=denorm(self.G_A2B(x)[0][0])\n                fake_B2A=denorm(self.G_B2A(y)[0][0])\n                fake_A2B=RGB2BGR(tensor2numpy(fake_A2B))\n                fake_B2A=RGB2BGR(tensor2numpy(fake_B2A))\n                cv2.imwrite(f\"{os.path.join(self.dir, self.dataset, 'test','testA2B' ,'A2B')}.{str(i).zfill(j)}.jpg\", fake_A2B * 255.0)\n                cv2.imwrite(f\"{os.path.join(self.dir, self.dataset, 'test','testB2A' ,'B2A')}.{str(i).zfill(j)}.jpg\", fake_B2A * 255.0)\n                \n#                 utils.make_grid(x, nrow=16, normalize=True)\n#                 utils.make_grid(y, nrow=16, normalize=True)\n#                 utils.make_grid(fake_A2B,nrow=16,normalize=True)\n#                 utils.make_grid(fake_B2A,nrow=16,normalize=True)\n#     #                 fake_A2B=torch.cat((x,fake_A2B),0)\n#     #                 fake_B2A=torch.cat((y,fake_B2A),0)\n#                 utils.save_image(fake_A2B,f\"{os.path.join(self.dir,self.dataset,'test','testA2B')}A2B_{str(i).zfill(j)}.jpg\")\n#                 utils.save_image(fake_B2A, f\"{os.path.join(self.dir, self.dataset,'test','testB2A')}B2A_{str(i).zfill(j)}.jpg\")\n","metadata":{"execution":{"iopub.status.busy":"2022-11-30T01:28:38.385594Z","iopub.execute_input":"2022-11-30T01:28:38.385863Z","iopub.status.idle":"2022-11-30T01:28:38.449799Z","shell.execute_reply.started":"2022-11-30T01:28:38.385839Z","shell.execute_reply":"2022-11-30T01:28:38.448826Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport torch, gc\nimport os\n#配置\ndef parse_args():\n    desc = \"Pytorch implementation of U-GAT-IT\"\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('--mode',type=str,default='test')\n    parser.add_argument('--light',type=bool,default=True)\n    parser.add_argument('--dataset',type=str,default='selfie2anime')\n    parser.add_argument('--data_path',type=str,default='/kaggle/input')\n    parser.add_argument('--batch_size',type=int,default=1)\n    parser.add_argument('--iter',type=int,default=1020000)\n    parser.add_argument('--lr',type=float,default=0.0001)\n    parser.add_argument('--decay_lr',type=float,default=3.952941175791991e-06)\n    parser.add_argument('--b1',type=float,default=0.5)\n    parser.add_argument('--b2',type=float,default=0.999)\n    parser.add_argument('--device',type=str,default='cuda',choices=['cpu','cuda'])\n    parser.add_argument('--img_size',type=int,default=256)\n    parser.add_argument('--channels',type=int,default=3)\n    parser.add_argument('--dir',type=str,default='results')\n    parser.add_argument('--print_freq',type=int,default='10000')\n    parser.add_argument('--weight_decay', type=float, default=0.0001, help='The weight decay')\n    parser.add_argument('--adv_weight', type=int, default=1, help='Weight for GAN')\n    parser.add_argument('--cycle_weight', type=int, default=10, help='Weight for Cycle')\n    parser.add_argument('--identity_weight', type=int, default=10, help='Weight for Identity')\n    parser.add_argument('--cam_weight', type=int, default=1000, help='Weight for CAM')\n    parser.add_argument('--path',type=str,default='dataset')\n    parser.add_argument('--test_path',type=str,default='/kaggle/input/model100/')\n    parser.add_argument('--retrain',type=bool,default=True)\n    parser.add_argument('--mutil_gpu',type=bool,default=False)\n    return check_args(parser.parse_args(args=[]))\ndef check_args(args):\n    check_folder(os.path.join(args.dir, args.dataset, 'model'))\n    check_folder(os.path.join(args.dir, args.dataset, 'img'))\n    check_folder(os.path.join(args.dir, args.dataset, 'test'))\n    check_folder(os.path.join(args.dir, args.dataset, 'test','testA2B'))\n    check_folder(os.path.join(args.dir, args.dataset, 'test','testB2A'))\n    return args\ndef main():\n   args=parse_args()\n   if args.mutil_gpu:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n        device_ids = [0, 1]\n   if args is None:\n       exit\n   gan=UGATIT(args)\n   gan.load_data()\n   if args.mode=='train':\n       if args.retrain:\n            gan.load_model()\n       print(f\"training on {args.device}\")\n       gan.train()\n       print(\"train haved finished\")\n   if args.mode=='test':\n       gan.test()\n       print(\"test haved finished\")\nif __name__==\"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2022-11-30T01:28:38.452406Z","iopub.execute_input":"2022-11-30T01:28:38.453096Z","iopub.status.idle":"2022-11-30T01:31:25.106231Z","shell.execute_reply.started":"2022-11-30T01:28:38.453062Z","shell.execute_reply":"2022-11-30T01:31:25.105238Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"读取模型！\n加载模型成功！\n加载学习率成功！\n","output_type":"stream"},{"name":"stderr","text":"100it [00:12,  8.30it/s]\n100it [00:05, 18.55it/s]\n100it [00:05, 19.12it/s]\n100it [00:05, 19.28it/s]\n100it [00:05, 19.70it/s]\n100it [00:05, 19.36it/s]\n100it [00:05, 19.31it/s]\n100it [00:05, 19.41it/s]\n100it [00:05, 19.81it/s]\n100it [00:05, 19.31it/s]\n100it [00:05, 19.83it/s]\n100it [00:05, 19.52it/s]\n100it [00:05, 19.48it/s]\n100it [00:05, 19.33it/s]\n100it [00:05, 19.76it/s]\n100it [00:05, 19.51it/s]\n100it [00:05, 19.76it/s]\n100it [00:05, 19.72it/s]\n100it [00:05, 19.50it/s]\n100it [00:05, 19.39it/s]\n100it [00:05, 19.40it/s]\n100it [00:05, 19.64it/s]\n100it [00:05, 19.47it/s]\n100it [00:05, 19.88it/s]\n100it [00:05, 19.57it/s]\n100it [00:05, 19.34it/s]\n100it [00:05, 19.45it/s]\n100it [00:05, 19.71it/s]\n100it [00:05, 19.40it/s]\n100it [00:05, 19.73it/s]","output_type":"stream"},{"name":"stdout","text":"test haved finished\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport zipfile\ndef file2zip(packagePath, zipPath):\n    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n    for path, dirNames, fileNames in os.walk(packagePath):\n        fpath = path.replace(packagePath, '')\n        for name in fileNames:\n            fullName = os.path.join(path, name)\n            name = fpath + '\\\\' + name\n            zip.write(fullName, name)\n    zip.close()\nif __name__ == \"__main__\":\n    # 文件夹路径\n    packagePath = './results/selfie2anime/test'\n    zipPath = './test8.zip'\n    if os.path.exists(zipPath):\n        os.remove(zipPath)\n    file2zip(packagePath, zipPath)\n    print(\"打包完成\")","metadata":{"execution":{"iopub.status.busy":"2022-11-30T01:31:25.107865Z","iopub.execute_input":"2022-11-30T01:31:25.108550Z","iopub.status.idle":"2022-11-30T01:31:29.452524Z","shell.execute_reply.started":"2022-11-30T01:31:25.108492Z","shell.execute_reply":"2022-11-30T01:31:29.451486Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"打包完成\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}